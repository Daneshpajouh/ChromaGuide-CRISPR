% ======================================================================
% CHAPTER 6: MAMBA ARCHITECTURE (SHORT VERSION)
% ======================================================================

\chapter{Mamba: Linear-Time Modeling of Megabase-Scale Chromatin}

To process the 1.2 Mbp context required for TAD-scale analysis, we replace the quadratic Transformer architecture ($O(N^2)$) with the linear-time Mamba State Space Model ($O(N)$).

\section{Selective State Space Models}
Mamba discretizes the continuous state space equation:
\begin{equation}
h'(t) = A h(t) + B x(t)
\end{equation}
Crucially, the discretization parameters $(\Delta, B, C)$ are \textbf{input-dependent}. This allows the model to "selectively" ignore noise (introns) and remember signal (enhancers).

\begin{figure}[h!]
    \centering
    \fbox{\parbox{0.9\textwidth}{\centering \vspace{1cm} \textbf{FIGURE PLACEHOLDER} \\ \textbf{File:} figures/fig\_6\_1.png \\ \textbf{Description:} The Selective Scan Mechanism. The model dynamically adjusts its "step size" \$\Delta\$ based on biolog... \vspace{1cm}}}
    \caption[Selective Scan]{The Selective Scan Mechanism. The model dynamically adjusts its "step size" $\Delta$ based on biological importance. Accessible regions (ATAC peaks) trigger fine-grained steps (high memory), while heterochromatin triggers coarse steps (low memory).}
    \label{fig:mamba_scan_short}
\end{figure}

\section{Computational Feasibility}
The efficiency gain is transformative:

\begin{table}[H]
\centering
\caption{Resource Requirements for 1.2 Mbp Sequence}
\label{tab:mamba_vs_transformer_short}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Architecture} & \textbf{Memory (Training)} & \textbf{Feasibility (Single A100)} \\
\hline
Transformer & 5.76 TB (Attention Matrix) & \textbf{Impossible} \\
\hline
Mamba & 3.7 GB (Hidden States) & \textbf{Feasible} \\
\hline
\end{tabular}
\end{table}

This efficiency allows us to train on thousands of guide RNAs with full genomic context, capturing long-range interactions that were previously computationally invisible.

\section{Epigenetically Modulated Memory}
We extend standard Mamba by modulating the step size $\Delta$ with epigenomic signals:
\begin{equation}
\Delta_t = \Delta_{base} \cdot (1 + \alpha \cdot \text{ATAC}_t)
\end{equation}
This forces the model to "pay more attention" (allocate more memory) to biologically active, accessible DNA regions, physically grounding the learning process.
