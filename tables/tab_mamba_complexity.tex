\begin{table}[H]
\centering
\caption{Mamba Complexity: Training vs Inference}
\label{tab:mamba_complexity}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Phase} & \textbf{Time Complexity} & \textbf{Space} & \textbf{Parallelizable} \\
\hline
Inference & $O(N \cdot d)$ & $O(d)$ & Sequential only \\
\hline
Training (parallel scan) & $O(N \cdot d / P + d \log N)$ & $O(N \cdot d)$ & Yes, $P$ parallel processors \\
\hline
Transformer forward & $O(N^2 \cdot d)$ & $O(N^2)$ & Embarrassingly parallel \\
\hline
Transformer training & $O(N^2 \cdot d)$ & $O(N^2 + B \cdot N \cdot d)$ & Yes but memory-limited \\
\hline
\end{tabular}
}
\end{table}