% ======================================================================
% CHAPTER 6: MAMBA STATE SPACE MODELS FOR LONG-CONTEXT GENOMICS
% Complete Mathematical Derivations and Genomics Applications
% ======================================================================

\chapter{Mamba State Space Models for Long-Context Genomics: Linear-Time Sequence Processing of Megabase-Scale Chromatin}

This chapter develops the mathematical theory and practical implementation of Mamba selective state space models for genomic sequence processing with massively extended context windows. Mamba~\cite{Gu2024} achieves linear $O(n \cdot d)$ time complexity versus Transformer quadratic $O(n^2 \cdot d)$, enabling practical processing of 1.2 Mbp (1.2 million base pair) genomic context—capturing complete TAD-scale chromatin structure while remaining computationally feasible on single GPUs. The chapter covers continuous-time state space foundations, discrete-time recurrence relations, selective state matrices, DNA-specific bidirectional processing, and adaptive memory mechanisms tailored to genomic data.

\section{State Space Models: Theoretical Foundations}

State space models form the mathematical basis for linear-time sequence modeling.

\subsection{Continuous-Time Linear Dynamical Systems}

\subsubsection{Definition and Notation}

A continuous-time linear state space model is defined by differential equations:

\begin{equation}
\frac{d\mathbf{x}(t)}{dt} = \mathbf{A}(t) \mathbf{x}(t) + \mathbf{B}(t) \mathbf{u}(t)
\end{equation}

\begin{equation}
\mathbf{y}(t) = \mathbf{C}(t) \mathbf{x}(t) + \mathbf{D}(t) \mathbf{u}(t)
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{x}(t) \in \mathbb{R}^d$: Hidden state vector at time $t$ (dimension $d$)
    
    \item $\mathbf{u}(t) \in \mathbb{R}^m$: Input at time $t$ (embedding dimension $m$)
    
    \item $\mathbf{y}(t) \in \mathbb{R}^n$: Output at time $t$
    
    \item $\mathbf{A}(t) \in \mathbb{R}^{d \times d}$: State transition matrix (can be time-varying)
    
    \item $\mathbf{B}(t) \in \mathbb{R}^{d \times m}$: Input projection matrix
    
    \item $\mathbf{C}(t) \in \mathbb{R}^{n \times d}$: Output projection matrix
    
    \item $\mathbf{D}(t) \in \mathbb{R}^{n \times m}$: Direct input-output connection (often zero)
\end{itemize}

\subsubsection{Time-Invariant Case}

For simplicity, assume constant matrices $\mathbf{A}, \mathbf{B}, \mathbf{C}, \mathbf{D}$ (time-invariant system):

\begin{equation}
\frac{d\mathbf{x}(t)}{dt} = \mathbf{A} \mathbf{x}(t) + \mathbf{B} \mathbf{u}(t)
\end{equation}

\begin{equation}
\mathbf{y}(t) = \mathbf{C} \mathbf{x}(t) + \mathbf{D} \mathbf{u}(t)
\end{equation}

\subsection{Solution to Continuous-Time ODE}

\subsubsection{Matrix Exponential Solution}

The solution to the continuous-time linear ODE can be expressed using the matrix exponential:

\begin{definition}[Matrix Exponential]
The matrix exponential $\exp(\mathbf{A} t)$ is defined as:

\begin{equation}
\exp(\mathbf{A} t) = \sum_{k=0}^{\infty} \frac{(\mathbf{A} t)^k}{k!} = \mathbf{I} + \mathbf{A} t + \frac{(\mathbf{A} t)^2}{2!} + \frac{(\mathbf{A} t)^3}{3!} + \cdots
\end{equation}

Key property (for diagonalizable $\mathbf{A}$ with eigenvalues $\lambda_i$):

\begin{equation}
\exp(\mathbf{A} t) = \sum_i \exp(\lambda_i t) \mathbf{v}_i \mathbf{v}_i^T
\end{equation}

where $\mathbf{v}_i$ are eigenvectors of $\mathbf{A}$.
\end{definition}

\subsubsection{Closed-Form Solution}

Solving the ODE with initial condition $\mathbf{x}(0) = \mathbf{x}_0$:

\begin{equation}
\mathbf{x}(t) = \exp(\mathbf{A} t) \mathbf{x}_0 + \int_0^t \exp(\mathbf{A}(t-\tau)) \mathbf{B} \mathbf{u}(\tau) d\tau
\end{equation}

For time-discrete input $\mathbf{u}_k$ applied at time $k$:

\begin{equation}
\mathbf{x}_{k+1} = \exp(\mathbf{A} \Delta t) \mathbf{x}_k + \int_0^{\Delta t} \exp(\mathbf{A} \tau) d\tau \mathbf{B} \mathbf{u}_k
\end{equation}

where $\Delta t$ is time step between discrete inputs.

\subsubsection{Discretization Approximation}

Define:
\begin{equation}
\mathbf{A}_d = \exp(\mathbf{A} \Delta t) \quad \text{(discrete state transition)}
\end{equation}

\begin{equation}
\mathbf{B}_d = (\mathbf{A}^{-1})(\mathbf{A}_d - \mathbf{I}) \mathbf{B} \quad \text{(discrete input projection)}
\end{equation}

Discrete recurrence relation:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{A}_d \mathbf{x}_k + \mathbf{B}_d \mathbf{u}_k
\end{equation}

\begin{equation}
\mathbf{y}_k = \mathbf{C} \mathbf{x}_k + \mathbf{D} \mathbf{u}_k
\end{equation}

This discrete recurrence is the basis for efficient sequential computation.

\section{Mamba: Selective State Space Models}

Standard SSMs have fixed $\mathbf{A}, \mathbf{B}, \mathbf{C}$ matrices independent of input. Mamba's innovation: make system matrices **input-dependent** through selective discretization.

\subsection{Selective Discretization Architecture}

\subsubsection{Learning Input-Dependent Parameters}

For each timestep $k$, predict model parameters from input:

\begin{equation}
[\Delta_k, \mathbf{B}_k, \mathbf{C}_k] = W(\mathbf{u}_k) \in \mathbb{R}^{1 + d + d}
\end{equation}

where $W$ is a learned neural network (typically 2-3 layer MLP) that maps input embedding to parameter predictions.

Specifically:
\begin{enumerate}
    \item \textbf{Step Size:} $\Delta_k \in \mathbb{R}^d$ (per-dimension step size, one scalar per state dimension)
    \begin{equation}
    \Delta_k = \text{softplus}(W_\Delta(\mathbf{u}_k))
    \end{equation}
    
    where softplus ensures positive step sizes: $\text{softplus}(x) = \log(1 + \exp(x))$
    
    \item \textbf{Input Projection:} $\mathbf{B}_k \in \mathbb{R}^{d}$ (input-dependent input projection)
    \begin{equation}
    \mathbf{B}_k = W_B(\mathbf{u}_k) \in \mathbb{R}^d
    \end{equation}
    
    \item \textbf{Output Projection:} $\mathbf{C}_k \in \mathbb{R}^{d}$ (input-dependent output projection)
    \begin{equation}
    \mathbf{C}_k = W_C(\mathbf{u}_k) \in \mathbb{R}^d
    \end{equation}
\end{enumerate}

The state transition matrix $\mathbf{A}$ remains fixed (learned during training, but not adapted per timestep).

\subsubsection{Discretization of State Transition}

Discretize the fixed state transition matrix using input-dependent step size:

\begin{equation}
\mathbf{A}_{d,k} = \exp(\Delta_k \mathbf{A})
\end{equation}

For diagonal $\mathbf{A}$ (recommended for efficiency), compute element-wise:

\begin{equation}
\mathbf{A}_{d,k}[i] = \exp(\Delta_k[i] \cdot \mathbf{A}[i, i])
\end{equation}

Discretize input projection:

\begin{equation}
\mathbf{B}_{d,k} = \Delta_k \odot \mathbf{B}_k
\end{equation}

where $\odot$ is element-wise multiplication.

\subsubsection{Recurrence Relation with Selective Discretization}

\begin{equation}
\mathbf{x}_k = \mathbf{A}_{d,k} \odot \mathbf{x}_{k-1} + \mathbf{B}_{d,k} \odot \mathbf{u}_k
\end{equation}

Output:

\begin{equation}
\mathbf{y}_k = \mathbf{C}_k \odot \mathbf{x}_k + \mathbf{D} \odot \mathbf{u}_k
\end{equation}

All operations are element-wise, enabling efficient computation in parallel.

\subsubsection{Computational Complexity Analysis}

\textbf{Per-Timestep Operations:}

\begin{enumerate}
    \item \textbf{Parameter prediction:} $O(d)$ (input projection through MLP)
    \item \textbf{Matrix exponential:} $O(d)$ (element-wise for diagonal $\mathbf{A}$)
    \item \textbf{Element-wise multiply-accumulate:} $O(d)$ (update $\mathbf{x}_k$)
    \item \textbf{Output computation:} $O(d)$ (element-wise with $\mathbf{C}_k$)
    \item \textbf{Total per-timestep:} $O(d)$
\end{enumerate}

\textbf{Sequence Processing:}

\begin{enumerate}
    \item Sequential processing: $N$ timesteps × $O(d)$ per timestep = $O(N \cdot d)$ total
    \item Linear memory: Only $O(d)$ hidden state needs storage, no quadratic attention matrix
    \item Linear in sequence length $N$ and dimension $d$
\end{enumerate}

\subsection{Biological Motivation for Input-Dependent Selectivity}

\subsubsection{Why Selectivity Matters for Genomics}

In genomic sequences, different positions have vastly different importance:

\begin{enumerate}
    \item \textbf{PAM Sites (critical):} PAM (NGG) sequences are critical landmarks for Cas9 targeting. When the model encounters a PAM site, it should allocate long memory to propagate this important information forward
    
    \item \textbf{Repetitive Regions (noise):} Repetitive DNA (satellite DNA, transposons) contains limited biological information. Short memory sufficient
    
    \item \textbf{Regulatory Elements (important):} Enhancers and promoters contain sequence motifs predicting gene regulation. Long memory enables learning distant enhancer-promoter interactions
    
    \item \textbf{Low-Information Regions (noise):} Intergenic regions with minimal biological function require short memory
\end{enumerate}

\subsubsection{Mathematical Formulation of Importance-Dependent Memory}

The input-dependent step size $\Delta_k$ controls memory length:

\begin{equation}
\text{Effective Memory Length} \propto -\frac{1}{\ln(\mathbf{A}_{d,k})}
\end{equation}

For stable SSM (eigenvalues of $\mathbf{A}$ negative), $\mathbf{A}_{d,k} \in (0, 1)$.

\begin{equation}
\text{High importance position:} \Delta_k \text{ large} \Rightarrow \mathbf{A}_{d,k} \text{ small} \Rightarrow \text{ long memory}
\end{equation}

\begin{equation}
\text{Low importance position:} \Delta_k \text{ small} \Rightarrow \mathbf{A}_{d,k} \text{ close to 1} \Rightarrow \text{ short memory}
\end{equation}

\subsubsection{Learning Importance-Dependent Selectivity}

The learned projection network $W_\Delta(\mathbf{u}_k)$ learns to predict step sizes based on input:

\begin{enumerate}
    \item \textbf{PAM sites (NGG):} When input encodes PAM pattern, $W_\Delta$ produces large $\Delta_k$ (long memory)
    
    \item \textbf{Epigenomic signals:} When ATAC signal or H3K27ac mark is strong, larger $\Delta_k$ (important regions get longer memory)
    
    \item \textbf{Repetitive sequences:} When input encodes repetitive patterns, smaller $\Delta_k$ (short memory, quickly forget)
\end{enumerate}

This learned selectivity enables the model to automatically allocate memory based on biological importance, without explicit programming.

\section{DNA-Specific Bidirectional Processing}

Genomic sequences are directional (5'→3' direction) but contain information in both directions.

\subsection{Bidirectional Mamba Architecture}

\subsubsection{Forward Mamba Pass}

Process sequence left-to-right (5'→3' direction):

\begin{equation}
\mathbf{h}^{(\text{fwd})}_k = \text{Mamba}_{\text{fwd}}(\mathbf{u}_{1:k})
\end{equation}

where $\mathbf{h}^{(\text{fwd})}_k$ summarizes information from positions $1$ to $k$.

\subsubsection{Backward Mamba Pass}

Process sequence right-to-left (reverse complementary direction):

\begin{equation}
\mathbf{h}^{(\text{bwd})}_k = \text{Mamba}_{\text{bwd}}(\mathbf{u}_{k:N})
\end{equation}

where $\mathbf{h}^{(\text{bwd})}_k$ summarizes information from positions $k$ to $N$ (in reverse order).

\subsubsection{Bidirectional Fusion}

Combine forward and backward representations:

\begin{equation}
\mathbf{h}_k^{(\text{merged})} = \mathbf{W}_{\text{fuse}}[\mathbf{h}^{(\text{fwd})}_k; \mathbf{h}^{(\text{bwd})}_k]
\end{equation}

where $[\cdot; \cdot]$ is concatenation and $\mathbf{W}_{\text{fuse}}$ is learned fusion matrix.

\subsubsection{Biological Motivation}

DNA contains information in both strands:

\begin{enumerate}
    \item \textbf{Forward Strand (5'→3'):} Guide RNA sequence specified relative to forward strand
    
    \item \textbf{Reverse Strand:} Complementary information; Cas9 can target either strand
    
    \item \textbf{Palindromic Sequences:} Some regulatory elements (TFBS) are palindromic; meaningful in both directions
    
    \item \textbf{Codon Usage:} Coding regions have directional bias; reverse complement has different properties
\end{enumerate}

Bidirectional processing captures these multi-directional patterns.

\section{Adaptive Memory for Epigenomic Context}

Mamba can modulate memory based on epigenomic signals, allocating longer-range memory to genomically important regions.

\subsection{ATAC-Modulated Step Size}

\subsubsection{Mechanism}

Instead of fixed step size prediction, make step size proportional to local accessibility:

\begin{equation}
\Delta_k = \Delta_{\text{base}} \cdot (1 + \alpha \cdot \text{ATAC}_k)
\end{equation}

where:
\begin{itemize}
    \item $\Delta_{\text{base}}$: Baseline step size
    \item $\text{ATAC}_k$: ATAC signal at position $k$ (normalized to $[0, 1]$)
    \item $\alpha$: Learned coefficient (positive, typically 0.5-2.0)
\end{itemize}

\subsubsection{Interpretation}

\begin{enumerate}
    \item \textbf{High ATAC (open chromatin):} $\Delta_k$ large → long-range memory allocated to this region
    
    \item \textbf{Low ATAC (closed chromatin):} $\Delta_k$ small → short-range memory
\end{enumerate}

Biologically sensible: open chromatin regions are more biologically active, warrant longer-range memory.

\subsection{Hi-C Contact-Modulated Memory}

\subsubsection{Mechanism}

Use Hi-C contact frequency to modulate memory:

\begin{equation}
\Delta_k = \Delta_{\text{base}} \cdot \left(1 + \beta \sum_{j} C(k, j) \exp(-|j - k| / \lambda)\right)
\end{equation}

where:
\begin{itemize}
    \item $C(k, j)$: Hi-C contact frequency between positions $k$ and $j$
    \item $\lambda$: Decay constant (typically 50-200 kbp)
    \item $\beta$: Learned coefficient
\end{itemize}

This enables the model to learn that regions with strong 3D contacts (brought into spatial proximity) should have shared memory.

\subsubsection{Interpretation}

\begin{enumerate}
    \item \textbf{Strong 3D Contact:} Positions $k$ and $j$ frequently contacted → allocate long memory at both positions
    
    \item \textbf{Weak Contact:} Positions rarely contacted → shorter memory
\end{enumerate}

\section{Parallel Scan Algorithm for Training Efficiency}

While inference requires sequential processing ($O(N \cdot d)$ time), training can be parallelized using parallel scan algorithms.

\subsection{Parallel Prefix Scan}

\subsubsection{Definition}

The recurrence relation:

\begin{equation}
\mathbf{x}_k = \mathbf{A}_{d,k} \odot \mathbf{x}_{k-1} + \mathbf{B}_{d,k} \odot \mathbf{u}_k
\end{equation}

can be rewritten as associative operation (for diagonal operations):

\begin{equation}
\mathbf{x}_k = (\mathbf{A}_{d,k} \odot \cdots \odot \mathbf{A}_{d,1}) \odot \mathbf{x}_0 + \sum_{j=1}^k (\mathbf{A}_{d,k} \odot \cdots \odot \mathbf{A}_{d,j+1}) \odot (\mathbf{B}_{d,j} \odot \mathbf{u}_j)
\end{equation}

This is a prefix scan (cumulative sum-like operation).

\subsubsection{Parallel Computation}

Prefix scans can be computed in parallel with $O(\log N)$ depth (parallel steps) and $O(N \log N)$ work (total operations):

\begin{enumerate}
    \item \textbf{Divide sequence} into chunks of size $\sqrt{N}$
    \item \textbf{Compute within-chunk} prefix scans in parallel (fast, $O(\sqrt{N} \log \sqrt{N})$ per chunk)
    \item \textbf{Propagate between chunks} (combine results across chunks)
    \item \textbf{Total time:} $O(\sqrt{N} \log N)$ parallel steps for $N$ sequence positions
\end{enumerate}

\subsection{Training vs Inference Complexity

\begin{table}[H]
\centering
\caption{Mamba Complexity: Training vs Inference}
\label{tab:mamba_complexity}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Phase} & \textbf{Time Complexity} & \textbf{Space} & \textbf{Parallelizable} \\
\hline
Inference & $O(N \cdot d)$ & $O(d)$ & Sequential only \\
\hline
Training (parallel scan) & $O(N \cdot d / P + d \log N)$ & $O(N \cdot d)$ & Yes, $P$ parallel processors \\
\hline
Transformer forward & $O(N^2 \cdot d)$ & $O(N^2)$ & Embarrassingly parallel \\
\hline
Transformer training & $O(N^2 \cdot d)$ & $O(N^2 + B \cdot N \cdot d)$ & Yes but memory-limited \\
\hline
\end{tabular}
\end{table}

\section{DNA Embedding and Input Processing}

\subsection{Nucleotide Encoding}

\subsubsection{One-Hot Encoding}

Standard 4-dimensional binary encoding:

\begin{equation}
\mathbf{e}_{\text{onehot}}(A) = [1, 0, 0, 0]
\end{equation}

\begin{equation}
\mathbf{e}_{\text{onehot}}(C) = [0, 1, 0, 0]
\end{equation}

\begin{equation}
\mathbf{e}_{\text{onehot}}(G) = [0, 0, 1, 0]
\end{equation}

\begin{equation}
\mathbf{e}_{\text{onehot}}(T) = [0, 0, 0, 1]
\end{equation}

\subsubsection{K-mer Embeddings}

Alternatively, use k-mer (substring) embeddings to capture sequence motifs directly:

\begin{equation}
\mathbf{e}_{\text{kmer}}(\text{sequence}_k) = W_{\text{embed}} \cdot \text{one-hot}(\text{sequence}_k) \in \mathbb{R}^{512}
\end{equation}

where $W_{\text{embed}}$ are learned embeddings mapping k-mers to fixed-dimensional vectors.

Advantages:
\begin{enumerate}
    \item Directly captures biologically meaningful motifs (e.g., PAM sequences, TFBS)
    \item Reduced sequence length (k-mer stride reduces length by factor of $k$)
    \item Learned representations can capture sequence similarity
\end{enumerate}

\subsection{Multi-Scale Feature Fusion}

\subsubsection{Architecture}

\begin{enumerate}
    \item \textbf{Input:} 1.2 Mbp genomic sequence
    
    \item \textbf{Multiple k-mer Embeddings:} Compute embeddings at multiple scales:
    \begin{itemize}
        \item 3-mers: Fine-grained sequence patterns
        \item 5-mers: Motif-level patterns (TFBS, PAM)
        \item 7-mers: Longer motifs (nucleosome positioning)
    \end{itemize}
    
    \item \textbf{Concatenate:} Combine multi-scale embeddings
    \begin{equation}
    \mathbf{e}_{\text{multiscale}} = [\mathbf{e}_{3\text{-mer}}; \mathbf{e}_{5\text{-mer}}; \mathbf{e}_{7\text{-mer}}] \in \mathbb{R}^{1536}
    \end{equation}
    
    \item \textbf{Projection:} Project to Mamba input dimension
    \begin{equation}
    \mathbf{u}_k = W_{\text{proj}} \cdot \mathbf{e}_{\text{multiscale}} + \mathbf{b} \in \mathbb{R}^{512}
    \end{equation}
\end{enumerate}

\subsubsection{Epigenomic Feature Concatenation}

\begin{equation}
\mathbf{u}_k = [\mathbf{e}_{\text{multiscale}}_k; \text{ATAC}_k; H3K27ac_k; \text{NucOcc}_k; \text{MethLevel}_k; \text{Hi-C contacts}_k]
\end{equation}

\subsection{Positional Encoding}

Unlike Transformers (which use absolute positional encodings), Mamba relies on sequential recurrence to implicitly encode position. However, explicit positional information can enhance learning:

\subsubsection{Relative Position Encoding}

Instead of absolute position, encode relative distance to target/off-target site:

\begin{equation}
\text{RelPos}_k = k - k_{\text{target}}
\end{equation}

Useful for learning that nearby positions are more relevant than distant positions.

\subsubsection{Logarithmic Distance Encoding}

For long-range interactions, use logarithmic distance:

\begin{equation}
\text{LogDist}_k = \log(|\text{RelPos}_k| + 1)
\end{equation}

Compresses large distances while preserving local fine-grained information.

\section{Integration with Epigenomics and Conformal Prediction}

\subsection{End-to-End Architecture}

\begin{enumerate}
    \item \textbf{Input:} Genomic sequence (1.2 Mbp) + epigenomic signals (ATAC, H3K27ac, Hi-C, nucleosomes, methylation)
    
    \item \textbf{Mamba Encoder:} 
    \begin{itemize}
        \item Bidirectional Mamba processing (forward + backward)
        \item Adaptive memory based on ATAC/Hi-C signals
        \item Multi-scale k-mer embeddings
        \item Output: Position-level representations
    \end{itemize}
    
    \item \textbf{On-Target Head:} Predict efficiency at target site
    \begin{equation}
    \hat{e} = \text{DenseNet}_{\text{on}}(\mathbf{h}_{\text{target}}) \in [0, 1]
    \end{equation}
    
    \item \textbf{Off-Target Head:} Predict cutting probability at all off-target sites
    \begin{equation}
    \hat{p}_i = \text{DenseNet}_{\text{off}}(\mathbf{h}_i) \in [0, 1]
    \end{equation}
    
    \item \textbf{Conformal Prediction:} Wrap both outputs with uncertainty
    \begin{equation}
    \text{Efficiency Interval} = [\hat{e} - q_{\text{on}}, \hat{e} + q_{\text{on}}]
    \end{equation}
    
    \begin{equation}
    \text{Off-Target Interval}_i = [\hat{p}_i - q_{\text{off}}, \hat{p}_i + q_{\text{off}}]
    \end{equation}
\end{enumerate}

\subsection{Joint Training Objective}

Multi-task learning with regularization:

\begin{equation}
L_{\text{total}} = L_{\text{on-target}} + \lambda_{\text{off}} \cdot L_{\text{off-target}} + \lambda_{\text{reg}} \cdot \text{Regularization}
\end{equation}

where:
\begin{itemize}
    \item $L_{\text{on-target}} = \text{MSE}(\hat{e}, e_{\text{true}})$: On-target efficiency prediction loss
    
    \item $L_{\text{off-target}} = \text{BCE}(\hat{p}_i, p_i^{\text{true}})$: Off-target binary classification loss
    
    \item $\text{Regularization} = L2(\text{weights}) + \text{Dropout}$: Standard ML regularization
\end{itemize}

\section{Computational Performance on Long Sequences}

\subsection{Memory Requirements}

\subsubsection{Mamba for 1.2 Mbp Context}

\begin{enumerate}
    \item \textbf{Sequence representation:} $N = 1.2 \times 10^6$ positions (1.2 Mbp)
    
    \item \textbf{Hidden state:} $\mathbf{x}_k \in \mathbb{R}^{512}$
    \begin{equation}
    \text{Memory per position} = 512 \times 4 \text{ bytes} = 2,048 \text{ bytes} = 2 \text{ KB}
    \end{equation}
    
    \item \textbf{Total hidden state storage:} $1.2 \times 10^6 \times 2 \text{ KB} = 2.4 \text{ GB}$
    
    \item \textbf{Model parameters:} $\approx 100-200$ million (typical medium-size Mamba)
    \begin{equation}
    \text{Parameter memory} = 200 \times 10^6 \times 4 \text{ bytes} = 800 \text{ MB}
    \end{equation}
    
    \item \textbf{Total GPU memory:} $2.4 \text{ GB (hidden)} + 0.8 \text{ GB (params)} + 0.5 \text{ GB (intermediate)} = 3.7 \text{ GB}$
\end{enumerate}

Fits comfortably on single A100 GPU (40 GB memory).

\subsubsection{Transformer for Comparison}

\begin{enumerate}
    \item \textbf{Attention weights:} $N \times N = (1.2 \times 10^6)^2 = 1.44 \times 10^{12}$ elements
    
    \begin{equation}
    \text{Attention memory} = 1.44 \times 10^{12} \times 4 \text{ bytes} = 5.76 \text{ TB}
    \end{equation}
    
    \item \textbf{Plus model parameters, activations:} Total $\approx 6-10$ TB
\end{enumerate}

Completely infeasible: would require 150-250 A100 GPUs just for attention matrix storage.

\subsection{Wall-Clock Time Comparisons}

\subsubsection{Mamba Inference (Single Sample)}

\begin{enumerate}
    \item \textbf{Compute:} $1.2 \times 10^6 \text{ positions} \times 512 \text{ ops/position} = 6.1 \times 10^8 \text{ operations}$
    
    \item \textbf{A100 GPU speed:} $\approx 100 \text{ TFLOPS} = 10^{14} \text{ ops/sec}$ (practical throughput)
    
    \item \textbf{Inference time:} $\frac{6.1 \times 10^8 \text{ ops}}{10^{14} \text{ ops/sec}} = 6.1 \times 10^{-6} \text{ sec} = 6 \text{ µsec}$ (theoretical)
    
    \item \textbf{With overhead:} $\approx 1 \text{ second}$ per sample (practical, accounting for I/O and latency)
\end{enumerate}

\subsubsection{Batch Processing}

\begin{enumerate}
    \item Batch of 100 samples: $100 \text{ samples} \times 1 \text{ sec} = 100 \text{ seconds} = 1.7 \text{ minutes}$
    
    \item Processing 10,000 guides: $\approx 2.8 \text{ hours}$ on single GPU
\end{enumerate}

\subsubsection{Training (Parallel Scan)}

\begin{enumerate}
    \item \textbf{Forward pass:} $O(N \cdot d / P)$ with $P$ processors (parallel)
    
    \item \textbf{Backward pass:} Similar to forward
    
    \item \textbf{Training time per epoch:} $\approx 1-2$ hours on single GPU with 1000 training samples
\end{enumerate}

\section{Comparison with Alternative Architectures}

\subsection{Performance Comparison: Mamba vs Transformer vs CNN}

\begin{table}[H]
\centering
\caption{Architecture Comparison for Long-Context Genomics}
\label{tab:architecture_comparison}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Property} & \textbf{CNN} & \textbf{Transformer} & \textbf{Mamba} & \textbf{Optimal} \\
\hline
Time Complexity & $O(N \cdot d)$ & $O(N^2 \cdot d)$ & $O(N \cdot d)$ & Linear \\
\hline
Space Complexity & $O(d)$ & $O(N^2)$ & $O(d)$ & Linear \\
\hline
Context Window (practical) & 500 bp & 400 bp & 1.2 Mbp & 1.2+ Mbp \\
\hline
Memory (1.2 Mbp) & 100 MB & 5.76 TB & 3.7 GB & <10 GB \\
\hline
Long-Range Capture & Poor & Good (attention) & Excellent (memory) & Excellent \\
\hline
Parallelizable Training & Excellent & Excellent & Good (scan) & Excellent \\
\hline
Interpretability & Fair (filters) & Good (attention) & Excellent (memory) & Excellent \\
\hline
Genomic Inductive Bias & None & None & Selective (proposed) & Strong \\
\hline
\end{tabular}
\end{table}

\subsection{When to Use Each Architecture}

\begin{enumerate}
    \item \textbf{CNNs:} Short sequences (<500 bp), when context window sufficient, interpretability via filter visualization
    
    \item \textbf{Transformers:} Medium sequences (400-2000 bp), when computational resources available, need attention interpretability
    
    \item \textbf{Mamba:} Long sequences (>1 Mbp), chromatin context critical, computational efficiency important, single GPU deployment
\end{enumerate}

For CRISPRO-MAMBA-X focusing on TAD-scale (100-250 kbp) and full context (1.2 Mbp), Mamba is the only practical choice.

\section{Implementation Details and Training Considerations}

\subsection{Hyperparameter Selection}

\begin{table}[H]
\centering
\caption{Recommended Mamba Hyperparameters for Genomics}
\label{tab:hyperparameters}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Hyperparameter} & \textbf{Recommended Value} & \textbf{Justification} \\
\hline
State dimension $d$ & 512-1024 & Balance expressiveness vs memory \\
\hline
Step size $\Delta$ range & [0.001, 1.0] & Cover short-term to long-term memory \\
\hline
State matrix $\mathbf{A}$ & Diagonal, negative eigenvalues & Stability + efficiency \\
\hline
Batch size & 32-128 & Memory constraints for long sequences \\
\hline
Learning rate & 1e-4 & Start conservative, warm up \\
\hline
Gradient clipping & 1.0 & Prevent exploding gradients \\
\hline
Dropout rate & 0.1-0.2 & Regularization \\
\hline
\end{tabular}
\end{table}

\subsection{Initialization Strategy}

\begin{enumerate}
    \item \textbf{$\mathbf{A}$ matrix:} Initialize with random negative eigenvalues (diagonal matrix with values in $[-2, -0.5]$)
    
    \item \textbf{$\mathbf{B}, \mathbf{C}$ matrices:} Normal initialization (zero mean, small variance)
    
    \item \textbf{Projection networks ($W_\Delta, W_B, W_C$):} Xavier initialization
\end{enumerate}

\subsection{Training Stability}

\begin{enumerate}
    \item \textbf{Gradient clipping:} Clip gradients by global norm to value 1.0
    
    \item \textbf{Layer normalization:} Apply before Mamba layer for numerical stability
    
    \item \textbf{Learning rate warmup:} Linear warmup for first 10\% of training steps
    
    \item \textbf{Monitoring:} Track hidden state norms; exponential growth indicates instability
\end{enumerate}

\section{Summary: Mamba for Genomics}

CRISPRO-MAMBA-X leverages Mamba selective state space models to enable:

\begin{enumerate}
    \item \textbf{Linear Time Complexity:} $O(N \cdot d)$ vs Transformer $O(N^2 \cdot d)$, enabling $10^6 \times$ acceleration
    
    \item \textbf{Long-Context Processing:} 1.2 Mbp genomic context capturing TAD-scale chromatin structure
    
    \item \textbf{Selective Memory:} Input-dependent step sizes allocate longer memory to biologically important regions (PAM sites, regulatory elements, accessible chromatin)
    
    \item \textbf{Bidirectional DNA Processing:} Forward and reverse strand processing capturing multi-directional sequence information
    
    \item \textbf{Epigenomic Integration:} ATAC/Hi-C modulated memory enables context-dependent learning
    
    \item \textbf{Computational Efficiency:} 1.2 Mbp context fits in 3.7 GB GPU memory vs 5.76 TB for Transformers
    
    \item \textbf{Training Parallelization:} Parallel scan algorithms enable efficient training despite sequential inference
\end{enumerate}

These properties make Mamba the enabling technology for CRISPRO-MAMBA-X's revolutionary integration of genomics and epigenomics at unprecedented scale.

\begin{thebibliography}{99}

\bibitem{Gu2024} Gu, A., Goel, K., \& Ré, C. (2024). Mamba: Linear-time sequence modeling with selective state spaces. In \textit{Proceedings of the 12th International Conference on Learning Representations (ICLR 2024)}. arXiv preprint arXiv:2312.08782.

\end{thebibliography}

\newpage
